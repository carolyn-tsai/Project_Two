{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from splinter import Browser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the URL that will be scraped to a variable\n",
    "# https://www.theinfatuation.com with filters for San Francisco dog friendly restaurants\n",
    "\n",
    "url = \"https://www.theinfatuation.com/san-francisco/guides/dog-friendly-restaurants-SF\"\n",
    "\n",
    "# Create function to get html from website using Beautiful Soup\n",
    "\n",
    "def getHTMLContent(link):\n",
    "    html = urlopen(link)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function created above and pass through the URL defined earlier\n",
    "\n",
    "content = getHTMLContent(url)\n",
    "\n",
    "# From the content collected, find all tags \"div\" with class \"spot-block__title-copy\"\n",
    "# Each of the restaurant data is found within the html tag and class\n",
    "\n",
    "rest_data = content.find_all(\"div\", class_=\"spot-block__title-copy\")\n",
    "\n",
    "# Check the amount of restaurants to see if data makes sense\n",
    "\n",
    "len(rest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop to go through the HTML\n",
    "# Test out code - print out all the restaurant names to see if data collected is correct\n",
    "# Check to see if the tag and class are correct for getting the restaurant name\n",
    "\n",
    "for restaurant in rest_data:\n",
    "    name = restaurant.find(\"h3\").text\n",
    "    category = restaurant.find(\"span\", class_=\"overview-bold\").text\n",
    "    print(name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also find the restaurant URL by looping through content and finding the correct tag\n",
    "\n",
    "for entry in rest_data:\n",
    "    partial_url = entry.find(\"a\", href=True)[\"href\"]\n",
    "    print(partial_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The address data is nested\n",
    "\n",
    "city = content.find_all(\"p\", class_=\"small\")\n",
    "\n",
    "# To get just the text, loop through all \"p\" tags with class \"small\" and get both children\n",
    "\n",
    "for line in city[0:2]:\n",
    "    street = line.text\n",
    "    \n",
    "    # Test to see if extraction worked\n",
    "    \n",
    "    print(street)\n",
    "\n",
    "# The above produces the address in two separate lines\n",
    "# To get the address in one line, use f-string and position of data\n",
    "\n",
    "street = city[0].text\n",
    "city_state = city[1].text\n",
    "\n",
    "# Combine the data \n",
    "\n",
    "address = f\"{street}. {city_state}\"\n",
    "\n",
    "# Display address to see if it is correct\n",
    "\n",
    "address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for initializing browser\n",
    "# Use chromdriver.exe\n",
    "\n",
    "def init_browser():\n",
    "    executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "\n",
    "    # For Mac Users:\n",
    "    # executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "\n",
    "    return Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From tests above, create one function to scrape all the necessary data\n",
    "\n",
    "def scrape(url):\n",
    "    \n",
    "    # Initialize browser (from function created earlier)\n",
    "\n",
    "    browser = init_browser()\n",
    "\n",
    "    # Create an empty dicitonary to store scraped dog restaurant data\n",
    "    \n",
    "    restaurants = {}\n",
    "    restaurants[\"name\"] = []\n",
    "    restaurants[\"address\"] = []\n",
    "    restaurants[\"category\"] = []\n",
    "    \n",
    "    browser.visit(url)\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    dog_html = browser.html\n",
    "    \n",
    "    dog_soup = BeautifulSoup(dog_html, \"html.parser\")\n",
    "    \n",
    "    rest_data = dog_soup.find_all(\"div\", class_=\"spot-block__title-copy\")\n",
    "    \n",
    "    # Create a loop to collect the restaurant data\n",
    "    \n",
    "    for restaurant in rest_data:\n",
    "        name = restaurant.find(\"h3\").text\n",
    "        category = restaurant.find(\"span\", class_=\"overview-bold\").text\n",
    "        partial_url = restaurant.find(\"a\", href=True)[\"href\"]\n",
    "        base_url = \"https://www.theinfatuation.com\"\n",
    "        get_address_url = base_url+partial_url\n",
    "        browser.visit(get_address_url)\n",
    "        time.sleep(3)\n",
    "        address_html = browser.html\n",
    "        address_soup = BeautifulSoup(address_html, \"html.parser\")\n",
    "        address_box = address_soup.find_all(\"p\", class_=\"small\")\n",
    "        street = address_box[0].text\n",
    "        city = address_box[1].text\n",
    "        address = f\"{street}, {city}\" \n",
    "        restaurants[\"name\"].append(name)\n",
    "        restaurants[\"address\"].append(address)\n",
    "        restaurants[\"category\"].append(category)\n",
    "    \n",
    "    browser.quit()\n",
    "    \n",
    "    return restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function above to scrape the site\n",
    "\n",
    "rest_df = scrape(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a Pandas Data Frame\n",
    "\n",
    "restaurant_data = pd.DataFrame.from_dict(rest_df)\n",
    "\n",
    "# Display the data frame\n",
    "\n",
    "restaurant_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see of all restaurant names were gathered\n",
    "\n",
    "len(restaurant_data[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a CSV file to be used later\n",
    "\n",
    "restaurant_data.to_csv(\"infat_dog_rest_initial.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
